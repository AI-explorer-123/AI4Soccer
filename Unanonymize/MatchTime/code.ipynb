{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有关词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看原词表信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "允许的token总数: 2909\n",
      "占tokenizer词表大小的百分比: 2.27%\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 设置环境\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 加载与模型相同的tokenizer\n",
    "tokenizer_ckpt = \"/home/jiayuanrao/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "    # 添加与模型中相同的特殊token\n",
    "    tokenizer.add_tokens([\"[PLAYER]\", \"[TEAM]\", \"[COACH]\",\n",
    "                         \"[REFEREE]\", \"([TEAM])\"], special_tokens=True)\n",
    "except Exception as e:\n",
    "    print(f\"加载tokenizer时出错: {e}\")\n",
    "    print(\"如果是权限问题，请确保有访问权限或使用本地可用的tokenizer\")\n",
    "\n",
    "# 加载足球相关词汇ID列表\n",
    "file_path = './soccer_words_llama3.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    token_ids_list = pkl.load(file)\n",
    "\n",
    "# 添加特殊token ID（与模型中相同）\n",
    "token_ids_list.append(128000)\n",
    "token_ids_list.append(128001)\n",
    "\n",
    "# 计算基本统计信息\n",
    "print(f\"允许的token总数: {len(token_ids_list)}\")\n",
    "print(f\"占tokenizer词表大小的百分比: {len(token_ids_list)/len(tokenizer)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建新词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokens...\n",
      "Found 942 annotation files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 942/942 [00:27<00:00, 34.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 6740\n",
      "Percentage of tokenizer vocabulary: 5.25%\n",
      "\n",
      "Sample tokens:\n",
      "Token ID: 12055, Text: 'umps'\n",
      "Token ID: 89502, Text: 'ambi'\n",
      "Token ID: 33766, Text: ' reflex'\n",
      "Token ID: 75696, Text: 'Fred'\n",
      "Token ID: 2970, Text: '58'\n",
      "Token ID: 12301, Text: 'hen'\n",
      "Token ID: 1474, Text: '-m'\n",
      "Token ID: 26235, Text: 'ieu'\n",
      "Token ID: 20940, Text: ' posit'\n",
      "Token ID: 70904, Text: ' bounced'\n",
      "Token ID: 1645, Text: ' ac'\n",
      "Token ID: 16820, Text: 'yt'\n",
      "Token ID: 21933, Text: ' striking'\n",
      "Token ID: 14797, Text: 'ovement'\n",
      "Token ID: 15916, Text: 'erts'\n",
      "Token ID: 43085, Text: '-ag'\n",
      "Token ID: 27561, Text: 'icol'\n",
      "Token ID: 21117, Text: '-foot'\n",
      "Token ID: 3729, Text: ' contact'\n",
      "Token ID: 23083, Text: 'David'\n",
      "\n",
      "Tokens saved to unanonymized_soccer_words_llama3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def collect_all_tokens(root_dir, tokenizer, timestamp_key=\"gameTime\"):\n",
    "    \"\"\"\n",
    "    遍历所有标注文件并收集所有token\n",
    "    \"\"\"\n",
    "    all_tokens = set()\n",
    "\n",
    "    # 进度条\n",
    "    all_files = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file in ['Labels-caption.json', 'Labels-caption_with_gt.json']:\n",
    "                all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "    print(f\"Found {len(all_files)} annotation files\")\n",
    "\n",
    "    for file_path in tqdm(all_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # 提取所有的文本\n",
    "            for annotation in data.get('annotations', []):\n",
    "                unanonymized = annotation.get('description', '')\n",
    "                if not unanonymized:\n",
    "                    continue\n",
    "\n",
    "                # 对每个文本进行tokenize\n",
    "                tokens = tokenizer(\n",
    "                    unanonymized,\n",
    "                    add_special_tokens=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_ids[0]\n",
    "\n",
    "                # 将token添加到集合中\n",
    "                all_tokens.update(tokens.tolist())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(list(all_tokens))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置tokenizer\n",
    "    tokenizer_ckpt = \"/home/jiayuanrao/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "    tokenizer.add_tokens(\n",
    "        [\"[PLAYER]\", \"[TEAM]\", \"[COACH]\", \"[REFEREE]\", \"([TEAM])\"],\n",
    "        special_tokens=True\n",
    "    )\n",
    "\n",
    "    # 数据根目录\n",
    "    ann_root = \"./dataset\"  # 请替换为实际的标注文件根目录\n",
    "\n",
    "    # 收集所有token\n",
    "    print(\"Collecting tokens...\")\n",
    "    all_tokens = collect_all_tokens(ann_root, tokenizer)\n",
    "\n",
    "    # 添加特殊token ID\n",
    "    all_tokens.extend([128000, 128001])\n",
    "\n",
    "    # 输出统计信息\n",
    "    print(f\"Total unique tokens: {len(all_tokens)}\")\n",
    "    print(\n",
    "        f\"Percentage of tokenizer vocabulary: {len(all_tokens)/len(tokenizer)*100:.2f}%\")\n",
    "\n",
    "    # 随机抽样一些token查看\n",
    "    sample_size = min(20, len(all_tokens))\n",
    "    sample_tokens = random.sample(all_tokens, sample_size)\n",
    "    print(\"\\nSample tokens:\")\n",
    "    for token in sample_tokens:\n",
    "        try:\n",
    "            print(f\"Token ID: {token}, Text: '{tokenizer.decode([token])}'\")\n",
    "        except:\n",
    "            print(f\"Token ID: {token}, Decode failed\")\n",
    "\n",
    "    # 保存为pkl文件\n",
    "    output_file = \"unanonymized_soccer_words_llama3.pkl\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pkl.dump(all_tokens, f)\n",
    "    print(f\"\\nTokens saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|end_of_text|>'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "    # 设置tokenizer\n",
    "tokenizer_ckpt = \"/home/jiayuanrao/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "tokenizer.decode([128000, 128001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatchTime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "from models.matchvoice_model import matchvoice_model\n",
    "%tb\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path, size=224, fps=2):\n",
    "        self.video_path = video_path\n",
    "        self.size = size\n",
    "        self.fps = fps\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((self.size, self.size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[\n",
    "                                 0.26862954, 0.26130258, 0.27577711]),\n",
    "        ])\n",
    "        # Load video using OpenCV\n",
    "        self.cap = cv2.VideoCapture(self.video_path)\n",
    "        self.length = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # Calculate frames to capture based on FPS\n",
    "        self.frame_indices = [int(x * self.cap.get(cv2.CAP_PROP_FPS) / self.fps)\n",
    "                              for x in range(int(self.length / self.cap.get(cv2.CAP_PROP_FPS) * self.fps))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.frame_indices[idx])\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error in reading frame\")\n",
    "            return None\n",
    "        # Convert color from BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Apply transformations\n",
    "        frame = self.transforms(Image.fromarray(frame))\n",
    "        return frame.to(torch.float16)\n",
    "\n",
    "    def close(self):\n",
    "        self.cap.release()\n",
    "\n",
    "\n",
    "def encode_features(data_loader, encoder, device):\n",
    "    all_features = None  # 初始化为None，用于第一次赋值\n",
    "    for frames in data_loader:\n",
    "        features = encoder(frames.to(device))\n",
    "        if all_features is None:\n",
    "            all_features = features  # 第一次迭代，直接赋值\n",
    "        else:\n",
    "            all_features = torch.cat(\n",
    "                (all_features, features), dim=0)  # 后续迭代，在第0维（行）上连接\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def predict_single_video_CLIP(video_path, predict_model, visual_encoder, size, fps, device):\n",
    "    # Loading features\n",
    "    try:\n",
    "        dataset = VideoDataset(video_path, size=size, fps=fps)\n",
    "        data_loader = DataLoader(\n",
    "            dataset, batch_size=40, shuffle=False, pin_memory=True, num_workers=0)\n",
    "        # print(\"Start encoding!\")\n",
    "        features = encode_features(data_loader, visual_encoder, device)\n",
    "        dataset.close()\n",
    "        print(\"Features of this video loaded with shape of:\", features.shape)\n",
    "    except:\n",
    "        print(\"Error with loading:\", video_path)\n",
    "\n",
    "    sample = {\n",
    "        \"features\": features.unsqueeze(dim=0),\n",
    "        \"labels\": None,\n",
    "        \"attention_mask\": None,\n",
    "        \"input_ids\": None\n",
    "    }\n",
    "\n",
    "    # Doing prediction:\n",
    "    comment = predict_model(sample)\n",
    "    print(\"The commentary is:\", comment)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Process video files for feature extraction.')\n",
    "parser.add_argument('--video_path', type=str, default=\"/remote-home/jiayuanrao/haokai/UniSoccer/train_data/video_clips/europe_uefa-champions-league_2016-2017/2017-04-12 - 21-45 Bayern Munich 1 - 2 Real Madrid/2_27_58.mp4\",\n",
    "                    help='Path to the soccer game video clip.')\n",
    "parser.add_argument('--device', type=str,\n",
    "                    default=\"cuda:2\", help='Device to extract.')\n",
    "parser.add_argument('--size', type=int, default=224,\n",
    "                    help='Size to which each video frame is resized.')\n",
    "parser.add_argument('--fps', type=int, default=2,\n",
    "                    help='Frames per second to sample from the video.')\n",
    "parser.add_argument(\"--tokenizer_name\", type=str, default=\"/home/jiayuanrao/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct\",\n",
    "                    help=\"LLM checkpoints, use path in your computer is fine as well\")\n",
    "parser.add_argument(\"--restricted_token_list_path\", type=str,\n",
    "                    default=\"./Restricted_Token_List/unanonymized_llama3.pkl\",\n",
    "                    help=\"Restricted token list, use path in your computer is fine as well\")\n",
    "parser.add_argument(\"--model_ckpt\", type=str, default=\"./results/unanonymized_matchtime_aligned_time/checkpoints/model_save_best_val_CIDEr.pth\",\n",
    "                    help=\"Model checkpoints, use path in your computer is fine as well\")\n",
    "parser.add_argument(\"--num_query_tokens\", type=int, default=32)\n",
    "parser.add_argument(\"--num_video_query_token\", type=int, default=32)\n",
    "parser.add_argument(\"--num_features\", type=int, default=512)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 创建并配置模型\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=args.device)\n",
    "model.eval()\n",
    "# print(model.dtype)\n",
    "clip_image_encoder = model.encode_image\n",
    "predict_model = matchvoice_model(llm_ckpt=args.tokenizer_name,\n",
    "                                    restricted_token_list_path=args.restricted_token_list_path,\n",
    "                                    tokenizer_ckpt=args.tokenizer_name,\n",
    "                                    num_video_query_token=args.num_video_query_token,\n",
    "                                    num_features=args.num_features,\n",
    "                                    device=args.device,\n",
    "                                    inference=True)\n",
    "# Load checkpoints\n",
    "other_parts_state_dict = torch.load(args.model_ckpt)\n",
    "new_model_state_dict = predict_model.state_dict()\n",
    "for key, value in other_parts_state_dict.items():\n",
    "    if key in new_model_state_dict:\n",
    "        new_model_state_dict[key] = value\n",
    "predict_model.load_state_dict(new_model_state_dict)\n",
    "predict_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = ''\n",
    "\n",
    "predict_single_video_CLIP(video_path=video_path, predict_model=predict_model,\n",
    "                          visual_encoder=clip_image_encoder, device=args.device, size=args.size, fps=args.fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_path = './features/features_CLIP/england_epl_2014-2015/2015-02-21 - 18-00 Chelsea 1 - 1 Burnley/1_224p_CLIP.npy'\n",
    "features = np.load(feature_path)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unanonymized Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_entities(reference, anonymized):\n",
    "    \"\"\"\n",
    "    根据 comments_text_anonymized 中的占位符，提取 reference 中相应位置的实体名称\n",
    "    支持提取所有 [PLAYER] 和 [TEAM] 标记对应的实际名称\n",
    "    \"\"\"\n",
    "    # 如果缺少任一输入，返回空列表\n",
    "    if not reference or not anonymized:\n",
    "        return [], []\n",
    "\n",
    "    # 处理特殊token的正则模式\n",
    "    player_pattern = r'\\[PLAYER\\]'\n",
    "    team_pattern = r'\\[TEAM\\]'\n",
    "    referee_pattern = r'\\[REFEREE\\]'\n",
    "    coach_pattern = r'\\[COACH\\]'\n",
    "\n",
    "    # 创建占位符标记的位置映射\n",
    "    placeholders = []\n",
    "    for match in re.finditer(player_pattern, anonymized):\n",
    "        placeholders.append((\"PLAYER\", match.start(), match.end()))\n",
    "    for match in re.finditer(team_pattern, anonymized):\n",
    "        placeholders.append((\"TEAM\", match.start(), match.end()))\n",
    "    for match in re.finditer(referee_pattern, anonymized):\n",
    "        placeholders.append((\"REFEREE\", match.start(), match.end()))\n",
    "    for match in re.finditer(coach_pattern, anonymized):\n",
    "        placeholders.append((\"COACH\", match.start(), match.end()))\n",
    "\n",
    "    # 按位置排序占位符\n",
    "    placeholders.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 记录参考文本中每个位置对应的原始文本和匿名文本的字符差异\n",
    "    offset_map = {}\n",
    "    ref_idx = 0\n",
    "    anon_idx = 0\n",
    "\n",
    "    while anon_idx < len(anonymized) and ref_idx < len(reference):\n",
    "        # 检查是否匹配到占位符\n",
    "        is_placeholder = False\n",
    "        placeholder_type = None\n",
    "        placeholder_length = 0\n",
    "\n",
    "        if anon_idx + 8 <= len(anonymized) and anonymized[anon_idx:anon_idx + 8] == \"[PLAYER]\":\n",
    "            placeholder_type = \"PLAYER\"\n",
    "            placeholder_length = 8\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 6 <= len(anonymized) and anonymized[anon_idx:anon_idx + 6] == \"[TEAM]\":\n",
    "            placeholder_type = \"TEAM\"\n",
    "            placeholder_length = 6\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 9 <= len(anonymized) and anonymized[anon_idx:anon_idx + 9] == \"[REFEREE]\":\n",
    "            placeholder_type = \"REFEREE\"\n",
    "            placeholder_length = 9\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 7 <= len(anonymized) and anonymized[anon_idx:anon_idx + 7] == \"[COACH]\":\n",
    "            placeholder_type = \"COACH\"\n",
    "            placeholder_length = 7\n",
    "            is_placeholder = True\n",
    "\n",
    "        if is_placeholder:\n",
    "            # 找到实体名\n",
    "            start_ref_idx = ref_idx\n",
    "            # 向前查找，直到遇到非名称字符\n",
    "            # 名称可能包含字母、空格和连字符\n",
    "            while ref_idx < len(reference) and (reference[ref_idx].isalpha() or reference[ref_idx] in [' ', '-']):\n",
    "                ref_idx += 1\n",
    "            # 记录偏移量和替换文本\n",
    "            offset_map[anon_idx] = {\n",
    "                \"type\": placeholder_type,\n",
    "                \"start\": start_ref_idx,\n",
    "                \"end\": ref_idx,\n",
    "                \"text\": reference[start_ref_idx:ref_idx].strip()\n",
    "            }\n",
    "            anon_idx += placeholder_length\n",
    "        else:\n",
    "            # 对于相同的字符，同步前进\n",
    "            if anon_idx < len(anonymized) and ref_idx < len(reference) and anonymized[anon_idx] == reference[ref_idx]:\n",
    "                anon_idx += 1\n",
    "                ref_idx += 1\n",
    "            else:\n",
    "                # 处理非占位符的不匹配字符\n",
    "                anon_idx += 1\n",
    "                ref_idx += 1\n",
    "\n",
    "    # 提取所有实体名\n",
    "    player_names = []\n",
    "    team_names = []\n",
    "\n",
    "    for placeholder_type, start, end in placeholders:\n",
    "        if start in offset_map:\n",
    "            entity_info = offset_map[start]\n",
    "            entity_text = entity_info[\"text\"]\n",
    "\n",
    "            if entity_text:\n",
    "                # 去除可能的标点和空格\n",
    "                entity_text = entity_text.strip()\n",
    "\n",
    "                # 处理不同类型的实体\n",
    "                if entity_info[\"type\"] == \"PLAYER\" and entity_text:\n",
    "                    # 只保留看起来像名字的部分\n",
    "                    name_parts = re.findall(\n",
    "                        r'[A-Z][a-zA-Z\\'.-]+(?:\\s+[A-Z][a-zA-Z\\'.-]+)*', entity_text)\n",
    "                    if name_parts:\n",
    "                        player_names.append(name_parts[0])\n",
    "                elif entity_info[\"type\"] == \"TEAM\" and entity_text:\n",
    "                    # 球队名可能是多个单词组成\n",
    "                    team_parts = re.findall(\n",
    "                        r'[A-Z][a-zA-Z\\'.-]+(?:\\s+[a-zA-Z\\'.-]+)*', entity_text)\n",
    "                    if team_parts:\n",
    "                        team_names.append(team_parts[0])\n",
    "\n",
    "    # 兜底方法：查找标准格式的球员名和球队名 \"Name (Team)\"\n",
    "    if not player_names or not team_names:\n",
    "        names_with_team = re.findall(\n",
    "            r'([A-Z][a-zA-Z\\'.-]+(?:\\s+[A-Z][a-zA-Z\\'.-]+)*)\\s*\\(([^)]+)\\)', reference)\n",
    "        if names_with_team:\n",
    "            if not player_names:\n",
    "                player_names = [name for name, _ in names_with_team]\n",
    "            if not team_names:\n",
    "                # 合并所有球队名并去重\n",
    "                all_teams = [team for _, team in names_with_team]\n",
    "                team_names = list(set(all_teams))\n",
    "\n",
    "    return player_names, team_names\n",
    "\n",
    "\n",
    "def entity_recall(prediction, reference, anonymized):\n",
    "    \"\"\"\n",
    "    计算在prediction中正确识别出的实体名字的比例\n",
    "    分别返回球员和球队的召回率\n",
    "    \"\"\"\n",
    "    gt_players, gt_teams = extract_entities(reference, anonymized)\n",
    "\n",
    "    # 在prediction中查找每个实体名（不区分大小写）\n",
    "    pred_lower = prediction.lower()\n",
    "\n",
    "    # 球员识别\n",
    "    player_hit = 0\n",
    "    matched_players = []\n",
    "    missed_players = []\n",
    "\n",
    "    for name in gt_players:\n",
    "        if name.lower() in pred_lower:\n",
    "            player_hit += 1\n",
    "            matched_players.append(name)\n",
    "        else:\n",
    "            missed_players.append(name)\n",
    "\n",
    "    # 球队识别\n",
    "    team_hit = 0\n",
    "    matched_teams = []\n",
    "    missed_teams = []\n",
    "\n",
    "    for name in gt_teams:\n",
    "        if name.lower() in pred_lower:\n",
    "            team_hit += 1\n",
    "            matched_teams.append(name)\n",
    "        else:\n",
    "            missed_teams.append(name)\n",
    "\n",
    "    return {\n",
    "        \"player_hit\": player_hit,  # 正确识别的球员数\n",
    "        \"player_total\": len(gt_players),  # 总球员数\n",
    "        \"matched_players\": matched_players,\n",
    "        \"missed_players\": missed_players,\n",
    "        \"team_hit\": team_hit,  # 正确识别的球队数\n",
    "        \"team_total\": len(gt_teams),  # 总球队数\n",
    "        \"matched_teams\": matched_teams,\n",
    "        \"missed_teams\": missed_teams,\n",
    "        \"gt_players\": gt_players,\n",
    "        \"gt_teams\": gt_teams\n",
    "    }\n",
    "\n",
    "\n",
    "def unanonymous_metric(inference_result_path):\n",
    "    \"\"\"\n",
    "    评估模型识别球员名字和球队名的能力\n",
    "    \"\"\"\n",
    "    with open(inference_result_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    pairs = data.get('pairs', [])\n",
    "\n",
    "    # 统计所有样本的总数\n",
    "    total_player_hit = 0  # 所有样本中正确识别的球员总数\n",
    "    total_player_count = 0  # 所有样本中球员总数\n",
    "    total_team_hit = 0  # 所有样本中正确识别的球队总数\n",
    "    total_team_count = 0  # 所有样本中球队总数\n",
    "\n",
    "    sample_count = 0  # 有实体的样本数\n",
    "    detailed_results = []\n",
    "\n",
    "    for item in pairs:\n",
    "        prediction = item.get('prediction', '')\n",
    "        reference = item.get('reference', '')\n",
    "        anonymized = item.get('anonymous_comment', '')\n",
    "\n",
    "        # 跳过没有必要字段的条目\n",
    "        if not (prediction and reference and anonymized):\n",
    "            continue\n",
    "\n",
    "        recall_info = entity_recall(prediction, reference, anonymized)\n",
    "\n",
    "        # 只记录有球员名或球队名的样本\n",
    "        has_entities = recall_info[\"player_total\"] > 0 or recall_info[\"team_total\"] > 0\n",
    "\n",
    "        if has_entities:\n",
    "            sample_count += 1\n",
    "\n",
    "            # 累加各项指标\n",
    "            total_player_hit += recall_info[\"player_hit\"]\n",
    "            total_player_count += recall_info[\"player_total\"]\n",
    "            total_team_hit += recall_info[\"team_hit\"]\n",
    "            total_team_count += recall_info[\"team_total\"]\n",
    "\n",
    "            # 计算单个样本的召回率（仅用于打印）\n",
    "            player_recall = recall_info[\"player_hit\"] / \\\n",
    "                recall_info[\"player_total\"] if recall_info[\"player_total\"] > 0 else 1.0\n",
    "            team_recall = recall_info[\"team_hit\"] / \\\n",
    "                recall_info[\"team_total\"] if recall_info[\"team_total\"] > 0 else 1.0\n",
    "\n",
    "            # print(f\"参考: {reference}\")\n",
    "            # print(f\"预测: {prediction}\")\n",
    "            # print(f\"GT球员: {recall_info['gt_players']}\")\n",
    "            # print(\n",
    "            #     f\"匹配球员: {recall_info['matched_players']} | Player Recall: {player_recall:.2f}\")\n",
    "            # print(f\"GT球队: {recall_info['gt_teams']}\")\n",
    "            # print(\n",
    "            #     f\"匹配球队: {recall_info['matched_teams']} | Team Recall: {team_recall:.2f}\\n\")\n",
    "\n",
    "            detailed_results.append({\n",
    "                \"reference\": reference,\n",
    "                \"prediction\": prediction,\n",
    "                \"ground_truth_players\": recall_info[\"gt_players\"],\n",
    "                \"matched_players\": recall_info[\"matched_players\"],\n",
    "                \"player_recall\": player_recall,\n",
    "                \"ground_truth_teams\": recall_info[\"gt_teams\"],\n",
    "                \"matched_teams\": recall_info[\"matched_teams\"],\n",
    "                \"team_recall\": team_recall\n",
    "            })\n",
    "\n",
    "    # 计算整体召回率\n",
    "    overall_player_recall = total_player_hit / \\\n",
    "        total_player_count if total_player_count > 0 else 1.0\n",
    "    overall_team_recall = total_team_hit / \\\n",
    "        total_team_count if total_team_count > 0 else 1.0\n",
    "    overall_recall = (overall_player_recall + overall_team_recall) / 2\n",
    "\n",
    "    if sample_count > 0:\n",
    "        print(f\"球员识别 - 总共: {total_player_count}, 正确识别: {total_player_hit}\")\n",
    "        print(f\"球队识别 - 总共: {total_team_count}, 正确识别: {total_team_hit}\")\n",
    "        print(f\"整体球员Recall: {overall_player_recall:.4f}\")\n",
    "        print(f\"整体球队Recall: {overall_team_recall:.4f}\")\n",
    "        print(f\"综合Recall: {overall_recall:.4f}\")\n",
    "        print(f\"(共{sample_count}条有实体的样本)\")\n",
    "\n",
    "        # 保存详细结果供进一步分析\n",
    "        # with open('entity_recognition_results.json', 'w', encoding='utf-8') as f:\n",
    "        #     json.dump({\n",
    "        #         \"player_recall\": overall_player_recall,\n",
    "        #         \"team_recall\": overall_team_recall,\n",
    "        #         \"overall_recall\": overall_recall,\n",
    "        #         \"total_samples\": sample_count,\n",
    "        #         \"total_player_count\": total_player_count,\n",
    "        #         \"total_player_hit\": total_player_hit,\n",
    "        #         \"total_team_count\": total_team_count,\n",
    "        #         \"total_team_hit\": total_team_hit,\n",
    "        #         \"detailed_results\": detailed_results\n",
    "        #     }, f, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        print(\"没有可评估的样本。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员识别 - 总共: 3237, 正确识别: 227\n",
      "球队识别 - 总共: 3193, 正确识别: 1229\n",
      "整体球员Recall: 0.0701\n",
      "整体球队Recall: 0.3849\n",
      "综合Recall: 0.2275\n",
      "(共3005条有实体的样本)\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "inference_result_path = './results/unanonymized_matchtime_1fps_aligned_time/final_validation_results.json'\n",
    "unanonymous_metric(inference_result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
