{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get run id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run name =  qwen-fine-tuning  id:  dypubzid\n",
      "run name =  qwen-fine-tuning  id:  7v3kzuer\n",
      "run name =  qwen_team_player_prompt_20250519_025340  id:  lar9qs4a\n",
      "run name =  qwen_team_prompt_20250519_025412  id:  0g3162ji\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\n",
    "    path=\"1649466799-xi-an-jiaotong-university/qwen-fine-tuning\")\n",
    "for i in runs:\n",
    "  print(\"run name = \",i.name,\" id: \", i.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建unanonymized词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先查看特殊token的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 151645]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "    # 设置tokenizer\n",
    "tokenizer_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "tokenizer.encode(\"<|im_start|>\", \"<|im_end|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokens...\n",
      "Found 3 annotation files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 6087\n",
      "Percentage of tokenizer vocabulary: 4.01%\n",
      "\n",
      "Sample tokens:\n",
      "Token ID: 78953, Text: ' sideline'\n",
      "Token ID: 13391, Text: ' ending'\n",
      "Token ID: 585, Text: 'ak'\n",
      "Token ID: 20527, Text: 'ishes'\n",
      "Token ID: 79630, Text: ' Rooney'\n",
      "Token ID: 1549, Text: ' again'\n",
      "Token ID: 3791, Text: 'ising'\n",
      "Token ID: 2453, Text: ' care'\n",
      "Token ID: 6050, Text: ' Mo'\n",
      "Token ID: 7604, Text: 'ady'\n",
      "Token ID: 11794, Text: ' snow'\n",
      "Token ID: 78379, Text: ' Toby'\n",
      "Token ID: 24821, Text: 'icates'\n",
      "Token ID: 11137, Text: ' causes'\n",
      "Token ID: 429, Text: ' that'\n",
      "Token ID: 36112, Text: ' jersey'\n",
      "Token ID: 6250, Text: ' Ver'\n",
      "Token ID: 1961, Text: 'oin'\n",
      "Token ID: 1256, Text: 'ector'\n",
      "Token ID: 74063, Text: 'olg'\n",
      "\n",
      "Tokens saved to ../words_world/qwen/unanonymized_matchtime.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle as pkl\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def collect_all_tokens(root_dir, tokenizer, timestamp_key=\"gameTime\"):\n",
    "    \"\"\"\n",
    "    遍历所有标注文件并收集所有token\n",
    "    \"\"\"\n",
    "    all_tokens = set()\n",
    "\n",
    "    # 进度条\n",
    "    all_files = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[-1] == '.json':\n",
    "                all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "    print(f\"Found {len(all_files)} annotation files\")\n",
    "\n",
    "    for file_path in tqdm(all_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # 提取所有的文本\n",
    "            for clip in data:\n",
    "                unanonymized = clip.get('comments_text', '')\n",
    "                if not unanonymized:\n",
    "                    continue\n",
    "\n",
    "                # 对每个文本进行tokenize\n",
    "                tokens = tokenizer(\n",
    "                    unanonymized,\n",
    "                    add_special_tokens=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_ids[0]\n",
    "\n",
    "                # 将token添加到集合中\n",
    "                all_tokens.update(tokens.tolist())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return sorted(list(all_tokens))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置tokenizer\n",
    "    tokenizer_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "\n",
    "    # 数据根目录\n",
    "    ann_root = \"../train_data/json/MatchTime\"  # 请替换为实际的标注文件根目录\n",
    "\n",
    "    # 收集所有token\n",
    "    print(\"Collecting tokens...\")\n",
    "    all_tokens = collect_all_tokens(ann_root, tokenizer)\n",
    "\n",
    "    # 添加特殊token ID\n",
    "    all_tokens.extend([151644, 151645])\n",
    "\n",
    "    # 输出统计信息\n",
    "    print(f\"Total unique tokens: {len(all_tokens)}\")\n",
    "    print(\n",
    "        f\"Percentage of tokenizer vocabulary: {len(all_tokens)/len(tokenizer)*100:.2f}%\")\n",
    "\n",
    "    # 随机抽样一些token查看\n",
    "    sample_size = min(20, len(all_tokens))\n",
    "    sample_tokens = random.sample(all_tokens, sample_size)\n",
    "    print(\"\\nSample tokens:\")\n",
    "    for token in sample_tokens:\n",
    "        try:\n",
    "            print(f\"Token ID: {token}, Text: '{tokenizer.decode([token])}'\")\n",
    "        except:\n",
    "            print(f\"Token ID: {token}, Decode failed\")\n",
    "\n",
    "    # 保存为pkl文件\n",
    "    output_file = \"../words_world/qwen/unanonymized_matchtime.pkl\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pkl.dump(all_tokens, f)\n",
    "    print(f\"\\nTokens saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试load adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f73aad9aca47459a4f40cfa8733eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Qwen2_5_VLForConditionalGeneration\n",
    "import peft\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "model.load_adapter('../results/qwen_team_player_prompt_20250519_025340/final_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算平均token和最多token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def calculate_token_stats(root_dir, tokenizer):\n",
    "    \"\"\"\n",
    "    遍历所有标注文件并计算评论的平均token数量和最大token数量\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    max_tokens = 0\n",
    "    num_comments = 0\n",
    "\n",
    "    # 进度条\n",
    "    all_files = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[-1] == '.json':\n",
    "                all_files.append(os.path.join(subdir, file))\n",
    "\n",
    "    print(f\"Found {len(all_files)} annotation files\")\n",
    "\n",
    "    for file_path in tqdm(all_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            # 提取所有的文本\n",
    "            for clip in data:\n",
    "                unanonymized = clip.get('comments_text', '')\n",
    "                if not unanonymized:\n",
    "                    continue\n",
    "\n",
    "                # 对每个文本进行tokenize\n",
    "                tokens = tokenizer(\n",
    "                    unanonymized,\n",
    "                    add_special_tokens=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_ids[0]\n",
    "\n",
    "                token_count = len(tokens)\n",
    "                total_tokens += token_count\n",
    "                max_tokens = max(max_tokens, token_count)\n",
    "                num_comments += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    avg_tokens = total_tokens / num_comments if num_comments > 0 else 0\n",
    "    return avg_tokens, max_tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置tokenizer\n",
    "    tokenizer_ckpt = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n",
    "\n",
    "    # 数据根目录\n",
    "    ann_root = \"../train_data/json/MatchTime\"  # 请替换为实际的标注文件根目录\n",
    "\n",
    "    # 计算token统计信息\n",
    "    print(\"Calculating token statistics...\")\n",
    "    avg_tokens, max_tokens = calculate_token_stats(ann_root, tokenizer)\n",
    "\n",
    "    # 输出统计信息\n",
    "    print(f\"Average tokens per comment: {avg_tokens:.2f}\")\n",
    "    print(f\"Maximum tokens in a single comment: {max_tokens}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference unanonymized metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_entities(reference, anonymized):\n",
    "    \"\"\"\n",
    "    根据 comments_text_anonymized 中的占位符，提取 reference 中相应位置的实体名称\n",
    "    支持提取所有 [PLAYER] 和 [TEAM] 标记对应的实际名称\n",
    "    \"\"\"\n",
    "    # 如果缺少任一输入，返回空列表\n",
    "    if not reference or not anonymized:\n",
    "        return [], []\n",
    "\n",
    "    # 处理特殊token的正则模式\n",
    "    player_pattern = r'\\[PLAYER\\]'\n",
    "    team_pattern = r'\\[TEAM\\]'\n",
    "    referee_pattern = r'\\[REFEREE\\]'\n",
    "    coach_pattern = r'\\[COACH\\]'\n",
    "\n",
    "    # 创建占位符标记的位置映射\n",
    "    placeholders = []\n",
    "    for match in re.finditer(player_pattern, anonymized):\n",
    "        placeholders.append((\"PLAYER\", match.start(), match.end()))\n",
    "    for match in re.finditer(team_pattern, anonymized):\n",
    "        placeholders.append((\"TEAM\", match.start(), match.end()))\n",
    "    for match in re.finditer(referee_pattern, anonymized):\n",
    "        placeholders.append((\"REFEREE\", match.start(), match.end()))\n",
    "    for match in re.finditer(coach_pattern, anonymized):\n",
    "        placeholders.append((\"COACH\", match.start(), match.end()))\n",
    "\n",
    "    # 按位置排序占位符\n",
    "    placeholders.sort(key=lambda x: x[1])\n",
    "\n",
    "    # 记录参考文本中每个位置对应的原始文本和匿名文本的字符差异\n",
    "    offset_map = {}\n",
    "    ref_idx = 0\n",
    "    anon_idx = 0\n",
    "\n",
    "    while anon_idx < len(anonymized) and ref_idx < len(reference):\n",
    "        # 检查是否匹配到占位符\n",
    "        is_placeholder = False\n",
    "        placeholder_type = None\n",
    "        placeholder_length = 0\n",
    "\n",
    "        if anon_idx + 8 <= len(anonymized) and anonymized[anon_idx:anon_idx + 8] == \"[PLAYER]\":\n",
    "            placeholder_type = \"PLAYER\"\n",
    "            placeholder_length = 8\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 6 <= len(anonymized) and anonymized[anon_idx:anon_idx + 6] == \"[TEAM]\":\n",
    "            placeholder_type = \"TEAM\"\n",
    "            placeholder_length = 6\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 9 <= len(anonymized) and anonymized[anon_idx:anon_idx + 9] == \"[REFEREE]\":\n",
    "            placeholder_type = \"REFEREE\"\n",
    "            placeholder_length = 9\n",
    "            is_placeholder = True\n",
    "        elif anon_idx + 7 <= len(anonymized) and anonymized[anon_idx:anon_idx + 7] == \"[COACH]\":\n",
    "            placeholder_type = \"COACH\"\n",
    "            placeholder_length = 7\n",
    "            is_placeholder = True\n",
    "\n",
    "        if is_placeholder:\n",
    "            # 找到实体名\n",
    "            start_ref_idx = ref_idx\n",
    "            # 向前查找，直到遇到非名称字符\n",
    "            # 名称可能包含字母、空格和连字符\n",
    "            while ref_idx < len(reference) and (reference[ref_idx].isalpha() or reference[ref_idx] in [' ', '-']):\n",
    "                ref_idx += 1\n",
    "            # 记录偏移量和替换文本\n",
    "            offset_map[anon_idx] = {\n",
    "                \"type\": placeholder_type,\n",
    "                \"start\": start_ref_idx,\n",
    "                \"end\": ref_idx,\n",
    "                \"text\": reference[start_ref_idx:ref_idx].strip()\n",
    "            }\n",
    "            anon_idx += placeholder_length\n",
    "        else:\n",
    "            # 对于相同的字符，同步前进\n",
    "            if anon_idx < len(anonymized) and ref_idx < len(reference) and anonymized[anon_idx] == reference[ref_idx]:\n",
    "                anon_idx += 1\n",
    "                ref_idx += 1\n",
    "            else:\n",
    "                # 处理非占位符的不匹配字符\n",
    "                anon_idx += 1\n",
    "                ref_idx += 1\n",
    "\n",
    "    # 提取所有实体名\n",
    "    player_names = []\n",
    "    team_names = []\n",
    "\n",
    "    for placeholder_type, start, end in placeholders:\n",
    "        if start in offset_map:\n",
    "            entity_info = offset_map[start]\n",
    "            entity_text = entity_info[\"text\"]\n",
    "\n",
    "            if entity_text:\n",
    "                # 去除可能的标点和空格\n",
    "                entity_text = entity_text.strip()\n",
    "\n",
    "                # 处理不同类型的实体\n",
    "                if entity_info[\"type\"] == \"PLAYER\" and entity_text:\n",
    "                    # 只保留看起来像名字的部分\n",
    "                    name_parts = re.findall(\n",
    "                        r'[A-Z][a-zA-Z\\'.-]+(?:\\s+[A-Z][a-zA-Z\\'.-]+)*', entity_text)\n",
    "                    if name_parts:\n",
    "                        player_names.append(name_parts[0])\n",
    "                elif entity_info[\"type\"] == \"TEAM\" and entity_text:\n",
    "                    # 球队名可能是多个单词组成\n",
    "                    team_parts = re.findall(\n",
    "                        r'[A-Z][a-zA-Z\\'.-]+(?:\\s+[a-zA-Z\\'.-]+)*', entity_text)\n",
    "                    if team_parts:\n",
    "                        team_names.append(team_parts[0])\n",
    "\n",
    "    # 兜底方法：查找标准格式的球员名和球队名 \"Name (Team)\"\n",
    "    if not player_names or not team_names:\n",
    "        names_with_team = re.findall(\n",
    "            r'([A-Z][a-zA-Z\\'.-]+(?:\\s+[A-Z][a-zA-Z\\'.-]+)*)\\s*\\(([^)]+)\\)', reference)\n",
    "        if names_with_team:\n",
    "            if not player_names:\n",
    "                player_names = [name for name, _ in names_with_team]\n",
    "            if not team_names:\n",
    "                # 合并所有球队名并去重\n",
    "                all_teams = [team for _, team in names_with_team]\n",
    "                team_names = list(set(all_teams))\n",
    "\n",
    "    return player_names, team_names\n",
    "\n",
    "\n",
    "def entity_recall(prediction, reference, anonymized):\n",
    "    \"\"\"\n",
    "    计算在prediction中正确识别出的实体名字的比例\n",
    "    分别返回球员和球队的召回率\n",
    "    \"\"\"\n",
    "    gt_players, gt_teams = extract_entities(reference, anonymized)\n",
    "\n",
    "    # 在prediction中查找每个实体名（不区分大小写）\n",
    "    pred_lower = prediction.lower()\n",
    "\n",
    "    # 球员识别\n",
    "    player_hit = 0\n",
    "    matched_players = []\n",
    "    missed_players = []\n",
    "\n",
    "    for name in gt_players:\n",
    "        if name.lower() in pred_lower:\n",
    "            player_hit += 1\n",
    "            matched_players.append(name)\n",
    "        else:\n",
    "            missed_players.append(name)\n",
    "\n",
    "    # 球队识别\n",
    "    team_hit = 0\n",
    "    matched_teams = []\n",
    "    missed_teams = []\n",
    "\n",
    "    for name in gt_teams:\n",
    "        if name.lower() in pred_lower:\n",
    "            team_hit += 1\n",
    "            matched_teams.append(name)\n",
    "        else:\n",
    "            missed_teams.append(name)\n",
    "\n",
    "    return {\n",
    "        \"player_hit\": player_hit,  # 正确识别的球员数\n",
    "        \"player_total\": len(gt_players),  # 总球员数\n",
    "        \"matched_players\": matched_players,\n",
    "        \"missed_players\": missed_players,\n",
    "        \"team_hit\": team_hit,  # 正确识别的球队数\n",
    "        \"team_total\": len(gt_teams),  # 总球队数\n",
    "        \"matched_teams\": matched_teams,\n",
    "        \"missed_teams\": missed_teams,\n",
    "        \"gt_players\": gt_players,\n",
    "        \"gt_teams\": gt_teams\n",
    "    }\n",
    "\n",
    "\n",
    "def unanonymous_metric(inference_result_path):\n",
    "    \"\"\"\n",
    "    评估模型识别球员名字和球队名的能力\n",
    "    \"\"\"\n",
    "    with open(inference_result_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    pairs = data.get('pairs', [])\n",
    "\n",
    "    # 统计所有样本的总数\n",
    "    total_player_hit = 0  # 所有样本中正确识别的球员总数\n",
    "    total_player_count = 0  # 所有样本中球员总数\n",
    "    total_team_hit = 0  # 所有样本中正确识别的球队总数\n",
    "    total_team_count = 0  # 所有样本中球队总数\n",
    "\n",
    "    sample_count = 0  # 有实体的样本数\n",
    "    detailed_results = []\n",
    "\n",
    "    for item in pairs:\n",
    "        prediction = item.get('prediction', '')\n",
    "        reference = item.get('reference', '')\n",
    "        anonymized = item.get('comments_text_anonymized', '')\n",
    "\n",
    "        # 跳过没有必要字段的条目\n",
    "        if not (prediction and reference and anonymized):\n",
    "            continue\n",
    "\n",
    "        recall_info = entity_recall(prediction, reference, anonymized)\n",
    "\n",
    "        # 只记录有球员名或球队名的样本\n",
    "        has_entities = recall_info[\"player_total\"] > 0 or recall_info[\"team_total\"] > 0\n",
    "\n",
    "        if has_entities:\n",
    "            sample_count += 1\n",
    "\n",
    "            # 累加各项指标\n",
    "            total_player_hit += recall_info[\"player_hit\"]\n",
    "            total_player_count += recall_info[\"player_total\"]\n",
    "            total_team_hit += recall_info[\"team_hit\"]\n",
    "            total_team_count += recall_info[\"team_total\"]\n",
    "\n",
    "            # 计算单个样本的召回率（仅用于打印）\n",
    "            player_recall = recall_info[\"player_hit\"] / \\\n",
    "                recall_info[\"player_total\"] if recall_info[\"player_total\"] > 0 else 1.0\n",
    "            team_recall = recall_info[\"team_hit\"] / \\\n",
    "                recall_info[\"team_total\"] if recall_info[\"team_total\"] > 0 else 1.0\n",
    "\n",
    "            # print(f\"参考: {reference}\")\n",
    "            # print(f\"预测: {prediction}\")\n",
    "            # print(f\"GT球员: {recall_info['gt_players']}\")\n",
    "            # print(\n",
    "            #     f\"匹配球员: {recall_info['matched_players']} | Player Recall: {player_recall:.2f}\")\n",
    "            # print(f\"GT球队: {recall_info['gt_teams']}\")\n",
    "            # print(\n",
    "            #     f\"匹配球队: {recall_info['matched_teams']} | Team Recall: {team_recall:.2f}\\n\")\n",
    "\n",
    "            detailed_results.append({\n",
    "                \"reference\": reference,\n",
    "                \"prediction\": prediction,\n",
    "                \"ground_truth_players\": recall_info[\"gt_players\"],\n",
    "                \"matched_players\": recall_info[\"matched_players\"],\n",
    "                \"player_recall\": player_recall,\n",
    "                \"ground_truth_teams\": recall_info[\"gt_teams\"],\n",
    "                \"matched_teams\": recall_info[\"matched_teams\"],\n",
    "                \"team_recall\": team_recall\n",
    "            })\n",
    "\n",
    "    # 计算整体召回率\n",
    "    overall_player_recall = total_player_hit / \\\n",
    "        total_player_count if total_player_count > 0 else 1.0\n",
    "    overall_team_recall = total_team_hit / \\\n",
    "        total_team_count if total_team_count > 0 else 1.0\n",
    "    overall_recall = (overall_player_recall + overall_team_recall) / 2\n",
    "\n",
    "    if sample_count > 0:\n",
    "        print(f\"球员识别 - 总共: {total_player_count}, 正确识别: {total_player_hit}\")\n",
    "        print(f\"球队识别 - 总共: {total_team_count}, 正确识别: {total_team_hit}\")\n",
    "        print(f\"整体球员Recall: {overall_player_recall:.4f}\")\n",
    "        print(f\"整体球队Recall: {overall_team_recall:.4f}\")\n",
    "        print(f\"综合Recall: {overall_recall:.4f}\")\n",
    "        print(f\"(共{sample_count}条有实体的样本)\")\n",
    "\n",
    "        # 保存详细结果供进一步分析\n",
    "        # with open('entity_recognition_results.json', 'w', encoding='utf-8') as f:\n",
    "        #     json.dump({\n",
    "        #         \"player_recall\": overall_player_recall,\n",
    "        #         \"team_recall\": overall_team_recall,\n",
    "        #         \"overall_recall\": overall_recall,\n",
    "        #         \"total_samples\": sample_count,\n",
    "        #         \"total_player_count\": total_player_count,\n",
    "        #         \"total_player_hit\": total_player_hit,\n",
    "        #         \"total_team_count\": total_team_count,\n",
    "        #         \"total_team_hit\": total_team_hit,\n",
    "        #         \"detailed_results\": detailed_results\n",
    "        #     }, f, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        print(\"没有可评估的样本。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员识别 - 总共: 2998, 正确识别: 51\n",
      "球队识别 - 总共: 2953, 正确识别: 2336\n",
      "整体球员Recall: 0.0170\n",
      "整体球队Recall: 0.7911\n",
      "综合Recall: 0.4040\n",
      "(共2790条有实体的样本)\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "inference_result_path = './inference_results/raw_team_prompt.json'\n",
    "unanonymous_metric(inference_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员识别 - 总共: 2998, 正确识别: 51\n",
      "球队识别 - 总共: 2953, 正确识别: 2348\n",
      "整体球员Recall: 0.0170\n",
      "整体球队Recall: 0.7951\n",
      "综合Recall: 0.4061\n",
      "(共2790条有实体的样本)\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "inference_result_path = './inference_results/raw_team_player_prompt.json'\n",
    "unanonymous_metric(inference_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员识别 - 总共: 2993, 正确识别: 246\n",
      "球队识别 - 总共: 2948, 正确识别: 1566\n",
      "整体球员Recall: 0.0822\n",
      "整体球队Recall: 0.5312\n",
      "综合Recall: 0.3067\n",
      "(共2785条有实体的样本)\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "inference_result_path = './inference_results/team_player_prompt.json'\n",
    "unanonymous_metric(inference_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "球员识别 - 总共: 2947, 正确识别: 263\n",
      "球队识别 - 总共: 2902, 正确识别: 1604\n",
      "整体球员Recall: 0.0892\n",
      "整体球队Recall: 0.5527\n",
      "综合Recall: 0.3210\n",
      "(共2742条有实体的样本)\n"
     ]
    }
   ],
   "source": [
    "# 执行评估\n",
    "inference_result_path = './inference_results/team_prompt.json'\n",
    "unanonymous_metric(inference_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2804"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('./inference_results/team_prompt.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    pairs = data.get('pairs', [])\n",
    "len(pairs)  # 输出样本数量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
